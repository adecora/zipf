## 11.11.1. Explaining assertions
Given a list of numbers, the function `total` returns the total:
```python
>>> total([1, 2, 3, 4])
10
```

The function only works on numbers:
```python
>>> total(['a', 'b', 'c'])
ValueError: invalid literal for int() wiht base 10: 'a'
```

Explain in words what the assertions in this function check, and for each one, give an example of input that will make assertion fail.
```python
def total(values):
	assert len(values) > 0
	for element in values:
		assert int(element)
	values = [int(element) for element in values]
	total = sum(values)
	assert total > 0
	return total
```

---

```python
assert len(values) > 0
```
It checks that the input list `values` receives have at least one element. If we pass an empty list `total([])` to `total` function this `assert` will fail.

```python
for element in values:
	assert int(element)
```
Checks that all element in the list can be turned into integers. `total([1, 'a', 4])` will make it fail.

```python
assert total > 0
```
Checks the value return by total must be greater than 0, if our list have some negative integers like `total([2, -5])` will  make it fail.

## 11.11.2. Rectangle normalization
A rectangle can ve described using a tuple of four cartesian coordinates `(x0, y0, x1, y1)`, where `(x0, y0)` represents the lower left corner and `(x1, y1)` the upper right. In order to do some calculations, suppose we need to be able to normalize rectangles so that the lower left corner is at the origin (i.e., `(x0, y0) = (0, 0)`) and the longest side is 1.0 units long. This function does that:

```python
def normalize_rectangles(rect):
	"""Normalizes a rectangle so that it is at the origin
	and 1.0 units long on its longest axis. Input should be
	(x0, y0, x1, y1), where (x0, y0) and (x1, y1) define the 
	lower left and upper right corners of the rectangle."""
	
	# insert preconditions
	x0, y0, x1, y1 = rect
	# insert preconditions
	
	dx = x1 - x0
	dy = y1 - y0
	if dx > dy:
		scaled = float(dx) / dy
		upper_x, upper_y = 1.0, scaled
	else:
		scaled = float(dx) / dy
		upper_x, upper_y = scaled, 1.0
	
	# insert postconditions here
	
	return (0, 0, upper_x, upper_y)
```

In order to answer the following questions, cut and paste the `normalize_rectangle` function into a new file called **geometry.py** (outside of your **zipf** project) and save that file in a new directory called **exercises**.

1. To ensure that the inputs to `normalize_rectangle` are valid, add **preconditions** to check that:
	-	`rect` contains 4 coordinates.
	```python
	assert len(rect) == 4, 'Rectangles must contain 4 coordinates'
	```

	- the width of the rectangle is a positive, non-zero value (i.e., `x0 < x1`), and
	```python
	assert x0 < x1, 'Rectangle invalid width'
	```

	- the height of the rectangle is a positive, non-zero value (i.e., `y0 < y1`).
	```python
	assert y0 < y1, 'Rectangle invalid height'
	```

2. If the normalization calculation has worked correctly, the new `x1` coordinate will lie between 0 and 1 (i.e., `0 < upper_x <= 1.0`). Add a **postcondition** to check that this is true. Do the same for the new `y1` coordinate, `upper_y`.
```python
assert 0 < upper_x <= 1.0, 'Calculated upper X coordinate invalid'
assert 0 < upper_y <= 1.0, 'Calculated upper Y coordinate invalid'
```

Running `normalize_rectangle` for a short, wide rectangle should pass your new preconditions and postconditions:
```python
import geometry

geometry.normalize_rectangle([2, 5, 3, 10])
```

```
(0, 0 , 0.2, 1.0)
```

but will fail for a tall, skinny rectangle:
```python
geometry.normalize_rectangle([20, 15, 30, 20])
```

```python
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/examples/geometry.py", line 21, in nomalize_rectangles
    assert 0 < upper_y <= 1.0, 'Calculated upper Y coordinate invalid'
AssertionError: Calculated upper Y coordinate invalid
```

3. Find and correct the source of the error in `normalize_rectangle`. Once fixed, you should be able to succesfully run `geometry.normalize_rectangle([20, 15, 30, 20])	`.
```python
if dx > dy:
	scaled = float(dy) / dx
	upper_x, upper_y = 1.0, scaled
else:
	scaled = float(dx) / dy
	upper_x, upper_y = scaled, 1.0
```

4. Write a unit test for tall, skinny rectangles and save it in a new filled called `test_geometry.py`. Run `pytest` to make sure the test passes.
```python
import geometry

import pytest

def test_tall_skinny():
    """Test normalization of a tall, skinnny rectangle."""
    rectangle = [20, 15, 25, 30]
    expected_result = (0.0, 0.0, pytest.approx(0.333, abs=0.01), 1.0)
    actual_result = geometry.normalize_rectangles(rectangle)
    assert expected_result == actual_result
```

5. Add a couple more unit test to `test_geometry.py`. Explain the rationale behind each test.
```python
def test_short_wide():
    """Test normalization of a short, wide rectangle."""
    rectangle = [20, 15, 30, 20]
    expected_result = (0.0, 0.0, 1.0, 0.5)
    actual_result = geometry.normalize_rectangles(rectangle)
    assert expected_result == actual_result

def test_negative_coordinates():
	"""Test rectangle normalization with negative coords."""
	rectangle = [-5, 2, -1, 4]
	expected_result = (0.0, 0.0, 1.0, 0.5)
	actual_result = geometry.normalize_rectangles(rectangle)
	assert expected_result == actual_result
```


## 11.11.3. Testing with randomness
Programs that rely on random numbers are impossible to test because there's (deliberately) no way to predict their output. Luckily, computer programs don't actually use random numbers: they use a **pseudo-random number generator** (PRNG) that produces values in a repeatable but unpredictable way. Given the same initial **seed**, a PRNG will always produce the same sequence of values. How can we use this fact when testing programs that rely on pseudo-random numbers?
```python
>>> import random
>>> random.seed(5)
>>> random.random()
0.6229016948897019
>>> random.random()
0.7417869892607294
>>> random().random()
0.7951935655656966
```

After checking the sequence of values with a fixed **seed** we can use that **seed** on our test.

## 11.11.4. Testing with relative error
If E is the expected result of a function and A is the actual value it produces, the **relative error** is `abs((A - E) / E)`. This means that if we expected the result of test to be 2, 1, and 0, and we catually get 2.1, 1.1, and 0.1 the relative errors are 5%, 10%, and infinity. Why does this seem counter-intuitive, and what might be a better way to measure error in this case?
It seems counter-intuitive because relative error is a measure of a single value, but in this case we are looking at a distributions of values: each result is off by 0.1 compared to a range of 0-2. A better measure might be the largest absolute error  `abs(A - E)` divided by the standard deviation of the data.